BERT、Prompt、Regex Rule和TF-IDF+ML优缺点对比:

1. BERT(预训练Transformer模型)
  理解：
    》它是一种基于深度学习和迁移学习的现代NLP模型代表。
    》Transformer架构：BERT的核心是Transformer的编码器部分，利用自注意力机制(Self-Attention)来同时计算序列中所有词之间的关系，完美捕捉上下文信息。
    》预训练+微调：
        1.预训练：在海量无标注文本上，通过两个任务进行训练（“掩码语言模型MLM”与“下一句预测NSP”）；
        2.微调：将预训练好的BERT模型在下游任务（如文本分类、问答）的少量标注数据上进行额外训练，使其适应特定任务。
  优点：
    1.强大的性能，在几乎所有NLP任务上都达到了革命性的性能提升；
    2.深度语义理解；
    3.强大的泛化能力。
  缺点：
    1.计算资源消耗大；
    2.虽然微调数据少，但是对预训练的数据需求和算力通常个人或小公司无法承担；
    3.黑盒模型，可理解性差

2. Prompt(提示学习)
  理解：
    》它是一种使用模型的新范式，而不是一个单独的模型，通常与大语言模型（LLM,如GPT、BERT）结合使用。
    》传统的“预训练-微调”范式需要为每个任务准备标注数据并更新模型参数。
    》核心思想：通过设计一个合适的模版，将下游任务“重塑”为模型在预训练时解决过的任务，从而激发训练模型本身已有的知识，无需或只需极少更新参数。
    》如情感分析任务：
      传统微调方式：输入 “这部电影很棒” -> 输出 “积极”。
      Prompt方式： 输入 “这部电影很棒。总体感受是：__” -> 输出：让模型在__处填入“积极”或“消极”（像一个完形填空）。
    优点：
      1.高效利用模型知识，避免微调可能带来的“灾难性遗忘”；
      2.少样本/零样本学习；
      3.统一范式，可用几乎相同的模式解决各种不同类型的NLP任务。
    缺点：
      1.提示词工程，需要大量人工调试和实验；
      2.结果不稳定；
      3.依赖基础模型。

3. Regex Rule
    理解：
      》正则表达式：是一种用于匹配字符串中特定模式的强大工具。
      》规则系统：开发者需要手动编写一系列if-else规则和正则表达式来匹配文本并执行操作（如提取、分类）。
    优点：
      1.精准可控；
      2.无需训练数据；
      3.高透明度和可解释性
    缺点：
      1.泛化能力极差；
      2.维护成本高；
      3.无法理解语义。

4. TF-IDF + 机器学习分类器 (TF-IDF_ML)
    理解：
      》是一种经典的传统机器学习文本分类 pipeline。
      》TF-IDF (词频-逆文档频率)： 这是一种文本特征提取方法，而不是模型。它将文本文档转换为数值向量。
      》ML (机器学习分类器)： 将TF-IDF得到的向量输入给一个传统的机器学习分类器进行训练和预测，最常用的包括：
        1.朴素贝叶斯 (Naive Bayes)： 基于贝叶斯定理，假设特征之间相互独立。
        2.支持向量机 (SVM)： 寻找一个超平面来最大化不同类别样本之间的间隔。
        3.逻辑回归 (Logistic Regression)： 一种线性分类模型。

    优点：
      1.简单高效，训练和预测速度非常快，计算资源消耗极低；
      2.可解释性强；
      3.小数据友好，不容易过拟合。
    缺点：
      1.语义鸿沟： 无法捕捉词汇的语义和上下文信息。它将单词视为独立的符号（“词袋模型”假设），无法理解“苹果公司”和“吃苹果”中的“苹果”区别。
      2.稀疏性： 产生的向量维度很高（词汇表大小）且非常稀疏（大部分为0），效率较低。
      3.性能天花板低： 在处理复杂任务（如情感分析、语义相似度计算）时，性能上限远低于深度学习方法。
