BERT、Prompt、Regex Rule和TF-IDF+ML优缺点对比:

1. BERT(预训练Transformer模型)
  理解：
    》它是一种基于深度学习和迁移学习的现代NLP模型代表。
    》Transformer架构：BERT的核心是Transformer的编码器部分，利用自注意力机制(Self-Attention)来同时计算序列中所有词之间的关系，完美捕捉上下文信息。
    》预训练+微调：
        1.预训练：在海量无标注文本上，通过两个任务进行训练（“掩码语言模型MLM”与“下一句预测NSP”）；
        2.微调：将预训练好的BERT模型在下游任务（如文本分类、问答）的少量标注数据上进行额外训练，使其适应特定任务。
  优点：
    1.强大的性能，在几乎所有NLP任务上都达到了革命性的性能提升；
    2.深度语义理解；
    3.强大的泛化能力。
  缺点：
    1.计算资源消耗大；
    2.虽然微调数据少，但是对预训练的数据需求和算力通常个人或小公司无法承担；
    3.黑盒模型，可理解性差

2. Prompt(提示学习)
  理解：
    》它是一种使用模型的新范式，而不是一个单独的模型，通常与大语言模型（LLM,如GPT、BERT）结合使用。
    》传统的“预训练-微调”范式需要为每个任务准备标注数据并更新模型参数。
    》核心思想：通过设计一个合适的模版，将下游任务“重塑”为模型在预训练时解决过的任务，从而激发训练模型本身已有的知识，无需或只需极少更新参数。
    》如情感分析任务：
      传统微调方式：输入 “这部电影很棒” -> 输出 “积极”。
      Prompt方式： 输入 “这部电影很棒。总体感受是：__” -> 输出：让模型在__处填入“积极”或“消极”（像一个完形填空）。
    优点：
      1.高效利用模型知识，避免微调可能带来的“灾难性遗忘”；
      2.少样本/零样本学习；
      3.统一范式，可用几乎相同的模式解决各种不同类型的NLP任务。
    缺点：
