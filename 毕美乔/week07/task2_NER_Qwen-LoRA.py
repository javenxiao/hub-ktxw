# -*- coding: utf-8 -*-
"""ner_Qwen_LoRA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nXiOaVnF84sAVeQKnBi_FtJbqnyz_J3W
"""

from google.colab import drive
drive.mount('/content/drive/')

import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer,
)
# pip install peft
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from tqdm import tqdm
import torch


# 数据加载和预处理
import json

def build_ner_examples(sentence_path, tag_path, output_path=None):
    """
    将 sentence.txt + tag.txt 转换为 LoRA 微调用的 instruction-style NER 样本
    """
    examples = []
    with open(sentence_path, 'r', encoding='utf-8') as f_sent, \
         open(tag_path, 'r', encoding='utf-8') as f_tag:
        sentences = [line.strip().split() for line in f_sent if line.strip()]
        tags = [line.strip().split() for line in f_tag if line.strip()]

    assert len(sentences) == len(tags), "句子数与标签数不一致！"

    for sent, tag_seq in zip(sentences, tags):
        entities = []
        current_entity = ""
        current_type = None

        # 遍历字符与标签
        for ch, tag in zip(sent, tag_seq):
            if tag.startswith("B-"):
                # 开始新实体
                if current_entity:
                    entities.append({"实体": current_entity, "类型": current_type})
                current_entity = ch
                current_type = tag.split("-")[1]
            elif tag.startswith("I-") and current_type == tag.split("-")[1]:
                current_entity += ch
            else:
                # O 或者标签不匹配，结束当前实体
                if current_entity:
                    entities.append({"实体": current_entity, "类型": current_type})
                    current_entity = ""
                    current_type = None

        # 最后一个实体别忘了加进去
        if current_entity:
            entities.append({"实体": current_entity, "类型": current_type})

        # 构造 instruction 格式
        example = {
            "instruction": "请从以下文本中抽取所有命名实体，输出实体名称及其类型（人名PER、地点LOC、机构ORG）：",
            "input": ''.join(sent),
            "output": json.dumps(entities, ensure_ascii=False)
        }
        examples.append(example)

    # 保存为 JSON 格式
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f_out:
            json.dump(examples, f_out, ensure_ascii=False, indent=2)

        print(f"✅ 已生成 {len(examples)} 条样本，保存至 {output_path}")

    return examples

def initialize_model_and_tokenizer(model_path):
    """初始化tokenizer和模型"""
    # 加载tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        use_fast=False,
        trust_remote_code=True,
        local_files_only=True
    )

    # 加载模型
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        dtype=torch.float16,  # 使用半精度减少内存占用
        device_map=None,
        local_files_only=True
    )

    return tokenizer, model

def process_func(example, tokenizer):
    """
    处理单个样本（用于LoRA微调大模型的实体识别任务）
    每个样本包含文本输入和对应的实体标注结果
    """
    # 示例输入：
    # example = {
    #     "instruction": "请从以下文本中抽取实体：",
    #     "input": "阿里巴巴集团总部位于杭州。",
    #     "output": "[{'实体': '阿里巴巴集团', '类型': 'ORG'}, {'实体': '杭州', '类型': 'LOC'}]"
    # }

    # ===== 系统提示词部分 =====
    instruction_text = (
        "<|im_start|>system\n"
        "进行中文命名实体识别任务，输出 JSON 数组 [{'实体': 'XXX', '类型': 'YYY'}]\n"
        "<|im_end|>\n"
    )

    # ===== 用户输入部分 =====
    user_prompt = (
        f"<|im_start|>user\n{example['instruction']} {example['input']}<|im_end|>\n"
        "<|im_start|>assistant\n")

    # 拼接完整的 instruction prompt
    full_prompt = instruction_text + user_prompt

    # Tokenize 输入部分（system + user）
    instruction = tokenizer(full_prompt, add_special_tokens=False)

    # Tokenize 输出部分（模型应答）
    response = tokenizer(f"{example['output']}", add_special_tokens=False)

    # 拼接最终序列
    input_ids = instruction["input_ids"] + response["input_ids"] + [tokenizer.eos_token_id]
    attention_mask = instruction["attention_mask"] + response["attention_mask"] + [1]

    # 构造 labels：system/user 部分的 token 忽略（标 -100），只在 assistant 部分计算 loss
    labels = [-100] * len(instruction["input_ids"]) + response["input_ids"] + [tokenizer.eos_token_id]


    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

def setup_lora(model):
    """设置LoRA配置并应用到模型"""
    config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        inference_mode=False,
        r=8,
        lora_alpha=32,
        lora_dropout=0.1
    )

    model = get_peft_model(model, config)
    model.print_trainable_parameters()

    return model

def setup_training_args():
    """设置训练参数"""
    return TrainingArguments(
        output_dir="drive/MyDrive/week07_homework/output_Qwen1.5_ner",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        logging_steps=100,
        do_eval=True,
        eval_steps=50,
        num_train_epochs=5,
        save_steps=50,
        learning_rate=1e-4,
        max_grad_norm=1.0,  # ✅ 开启梯度裁剪
        fp16=False,       # ❌ 禁止 fp16
        bf16=False,        # ✅ 用 bf16（更稳）
        save_on_each_node=True,
        gradient_checkpointing=True,
        report_to="none"  # 禁用wandb等报告工具
    )

def predict(model, tokenizer, text, device='cpu'):
    """预测单个文本的意图"""
    # messages = [
    #     {"role": "system", "content": "进行中文命名实体识别任务，输出 JSON 数组 [{'实体': 'XXX', '类型': 'YYY'}]"},
    #     {"role": "user", "content": f"请从以下文本中抽取所有命名实体，输出实体名称及其类型（人名PER、地点LOC、机构ORG）： {text}"}
    # ]
    #
    # # 应用聊天模板
    # formatted_text = tokenizer.apply_chat_template(
    #     messages,
    #     tokenize=False,
    #     add_generation_prompt=True
    # )

    formatted_text = (
        "<|im_start|>system\n"
        "进行中文命名实体识别任务，输出 JSON 数组 [{'实体': 'XXX', '类型': 'YYY'}]\n"
        "<|im_end|>\n"
        "<|im_start|>user\n"
        f"请从以下文本中抽取所有命名实体，输出实体名称及其类型（人名PER、地点LOC、机构ORG）： {text}<|im_end|>\n"
        "<|im_start|>assistant\n"
    )

    # Tokenize输入
    model_inputs = tokenizer([formatted_text], return_tensors="pt").to(device)

    # 生成预测
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            do_sample=False,
            temperature=0.2,  # 降低温度以获得更确定的输出
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )



    # 提取生成的文本（去掉输入部分）
    generated_ids = generated_ids[:, model_inputs.input_ids.shape[1]:]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return response.strip()

def batch_predict(model, tokenizer, test_texts, device='cuda'):
    """批量预测测试集的意图"""
    pred_labels = []

    for text in tqdm(test_texts, desc="实体识别"):
        try:
            pred_label = predict(model, tokenizer, text, device)
            pred_labels.append(pred_label)
        except Exception as e:
            print(f"识别文本 '{text}' 时出错: {e}")
            pred_labels.append("")  # 出错时添加空字符串

    return pred_labels

# 主函数
def main():
    """主执行函数"""
    # 1. 加载数据
    print("加载数据...")
    # 示例调用
    train_examples = build_ner_examples("drive/MyDrive/week07_homework/msra/train/sentences.txt",
                       "drive/MyDrive/week07_homework/msra/train/tags.txt",
                       "drive/MyDrive/week07_homework/msra/train_ner_instruction_data.json")
    test_examples = build_ner_examples("drive/MyDrive/week07_homework//msra/test/sentences.txt",
                       "drive/MyDrive/week07_homework/msra/test/tags.txt",
                       "drive/MyDrive/week07_homework/msra/test_ner_instruction_data.json")

    # 2. 初始化模型和tokenizer
    print("初始化模型和tokenizer...")
    model_path = "drive/MyDrive/week07_homework//models/Qwen/Qwen3-0.6B"
    tokenizer, model = initialize_model_and_tokenizer(model_path)

    # 3. 处理数据
    print("处理训练数据...")
    process_func_with_tokenizer = lambda example: process_func(example, tokenizer)
    # 4. 划分训练集和验证集
    train_ds = Dataset.from_list(train_examples[:400])
    train_tokenized = train_ds.map(process_func_with_tokenizer)

    eval_ds = Dataset.from_list(test_examples[-200:])
    eval_tokenized = eval_ds.map(process_func_with_tokenizer)

    # 5. 设置LoRA
    print("设置LoRA...")
    model.enable_input_require_grads()
    model = setup_lora(model)

    # 6. 配置训练参数
    print("配置训练参数...")
    training_args = setup_training_args()

    # 7. 创建Trainer并开始训练
    print("开始训练...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
        data_collator=DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            padding=True,
            pad_to_multiple_of=8  # 优化GPU内存使用
        ),
    )

    trainer.train()

    # 8. 保存模型
    print("保存模型...")
    trainer.save_model()
    tokenizer.save_pretrained("drive/MyDrive/week07_homework/output_Qwen1.5_ner")

def sanitize_text(text: str) -> str:
    """替换容易触发 tokenizer/模型控制符的字符"""
    replacements = {
        '[': '【',
        ']': '】',
        '<': '〈',
        '>': '〉',
        '|': '｜'
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    return text

def test_examples(sentences):
    # 下载模型
    # modelscope download --model Qwen/Qwen3-0.6B  --local_dir Qwen/Qwen3-0.6B
    model_path = "drive/MyDrive/week07_homework/models/Qwen/Qwen3-0.6B"
    lora_path = "drive/MyDrive/week07_homework/output_Qwen1.5_ner"
    tokenizer, model = initialize_model_and_tokenizer(model_path)

    print("Tokenizer path:", tokenizer.name_or_path)
    print("Tokenizer vocab size:", tokenizer.vocab_size)
    print("Special tokens:", tokenizer.special_tokens_map)

    # 加载 LoRA 适配器
    model = PeftModel.from_pretrained(model, lora_path)
    # print(model)

    #（可选）合并权重后推理
    model = model.merge_and_unload()
    # print(model)
    print(tokenizer.is_fast)
    print(tokenizer.total_vocab_size)

    model.eval()
    model = model.float()
    model.cpu()

    # 测试预测
    for test_text in sentences:
        test_text = sanitize_text(test_text)

        result = predict(model, tokenizer, test_text)
        print(f"输入: {test_text}")
        print(f"识别实体: {result}")

if __name__ == "__main__":
    # 执行主函数
    # result_df = main()

    test_sentences = [
        '今天我约了王浩在恭王府吃饭，晚上在天安门逛逛。',
        '人工智能是未来的希望，也是中国和美国的冲突点。',
        '明天我们一起在海淀吃个饭吧，把叫刘涛和王华也叫上。',
        '同煤集团同生安平煤业公司发生井下安全事故 19名矿工遇难',
        '山东省政府办公厅就平邑县玉荣商贸有限公司石膏矿坍塌事故发出通报',
        '[新闻直播]间黑龙江:龙煤集团一煤矿发生火灾事故'
    ]

    # 单独测试
    test_examples(test_sentences)
